---
layout: post
title:  Dense Neural Networks (DNN) vs Recurrent Neural Networks (RNN)
read: 15
description: A mathematical description of the recurrent neural network (RNN) and the Long Short-term Memory cell (LSTM)
categories: machine learning
---

---

![](/assets/images/posts/6_RNN_LSTM/post.jpg)

Neural network is a very popular machine learning algorithm which has gained enormous traction with the introduction of <a href="https://www.nature.com/articles/nature14539" target="_blank">deep learning</a>.


# Dense Neural Networks {#DNN}

<figure>
<img style="width: 50%" src="/assets/images/posts/8_DNN_RNN/DNN.png" alt="DNN unit - neuron">
<figcaption style="text-align: center">A dense neural network unit - neuron</figcaption>
</figure>

The figure above shows a basic neural network unit - a neuron. 

In a neuron, inputs are linearly combined using a set of weights and a bias. The aggregated input is then passed through an activation function to generate an output. The <a href="https://pure.strath.ac.uk/ws/portalfiles/portal/118946797/Nwankpa_etal_ICCST_2021_Activation_functions_comparison_of_trends_in_practice.pdf " target="_blank">activation function</a> is typically a non-linear function. This is to introduce non-linearity in the neuron to learn more complex mapping from the inputs to the output. Multiple neurons can be combined together to provide multiple outputs, with each neuron linearly combining the inputs with different weightings to do so. The outputs can be added together or left as a multi-dimensional vector. This network of neurons is called a <a href="https://link.springer.com/article/10.1023/A:1007662407062" target="_blank">perceptron</a>.

Perceptrons can be stacked on top of each other, with the outputs of a perceptron being the inputs of another perceptron and so on. This is a dense neural network, where there are a network of weights and biases combining the inputs in complex ways to provide a final output from the last perceptron. This enables the model to learn complex mappings from the inputs to the final output. These weights and biases are typically learned through optimisation. The most popular method for optimisation is gradient descent with error backpropagation. You can learn more about it in the <a href="https://ieeexplore.ieee.org/document/6302929" target="_blank">original paper</a> if you are interested.

A dense neural network, although powerful, is not capable of accurately modelling sequential data like a time-series. It does not have the notion of order and time as it treats the inputs as independent. It is therefore not reliable for time-series data like flight engine data used for engine health monitoring. In order to model a sequence, the network needs to somehow 'remember' what happened in the past. 

Recurrent neural networks (RNN) have special structures implemented which allow the network to do so \cite{LSTM}. For example, the cell state in an RNN cell enables the model to memorise what has happened in the past and forget gate allows the discard of insignificant information. Therefore, an RNN is more suitable for handling a sequence-to-sequence mapping.

# Recurrent Neural Network (RNN) {#RNN}

<figure>
<img src="/assets/images/posts/8_DNN_RNN/DNN_RNN.png" alt="DNN vs RNN units">
<figcaption style="text-align: center">Comparison between a DNN unit and an RNN unit</figcaption>
</figure>

The figure above shows the neuron (on the left) and a basic recurrent neural network unit (on the right) - recurrent neuron. 

The recurrent neuron is similar to a simple neuron except that it has a feedback from the output back to the input. That is, the output from the previous time step is aggregated with the input at the current time step. This enables the network to 'know' what happened in previous time steps and thus enables a more accurate mapping and modelling of sequential data. Just like dense neural networks, multiple recurrent neurons can take in the same inputs and aggregate them in different ways to give different outputs, just like a perceptron. These recurrent neurons can be stacked up into layers to form the recurrent neural network.

# Conclusion

1. Recurrent neural networks (RNNs) are commonly used in sequence modelling due to a hidden state being introduced to encapsulate past information in a sequence.
2. Long Short-term Memory cells (LSTMs) introduce 3 gate mechanisms and an additional cell state to address the issue of vanishing gradient with RNN.
3. Some variants of LSTMs include the bidirectional LSTM, uni-LSTM with context, and the LSTMP architectures.
   
---

**Author**: <a href="https://github.com/andreusjh99" target="_blank">Chia Jing Heng (andreusjh99)</a>